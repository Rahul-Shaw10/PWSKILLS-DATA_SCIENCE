{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###[Q1.] What is a projection and how is it used in PCA?\n",
        "#####[Ans]\n",
        "Projection in PCA refers to transforming the data points onto a new subspace defined by the principal components. Each principal component is a linear combination of the original features, representing the directions of maximum variance in the data. By projecting data onto these components, PCA reduces dimensionality while retaining as much variance as possible.\n",
        "\n",
        "###[Q2.] How does the optimization problem in PCA work, and what is it trying to achieve?\n",
        "#####[Ans]\n",
        "PCA solves an optimization problem to find the principal components by:\n",
        "\n",
        "1. Maximizing the variance of the projected data onto each component.\n",
        "2. Ensuring that the components are orthogonal to one another.\n",
        "\n",
        "The goal is to identify directions (eigenvectors) that capture the most information (variance) in the data while reducing redundancy.\n",
        "\n",
        "###[Q3.] What is the relationship between covariance matrices and PCA?\n",
        "#####[Ans]\n",
        "The covariance matrix represents the pairwise variances and covariances of the features in the dataset. PCA uses this matrix to:\n",
        "\n",
        "1. Identify directions of maximum variance.\n",
        "2. Compute the eigenvectors (principal components) and eigenvalues (explained variance) of the covariance matrix.\n",
        "\n",
        "The eigenvectors indicate the directions of the principal components, and the eigenvalues measure their importance.\n",
        "\n",
        "###[Q4.] How does the choice of the number of principal components impact the performance of PCA?\n",
        "#####[Ans]\n",
        "1. Too Few Components: Loss of information, leading to underfitting.\n",
        "2. Too Many Components: Retains noise and redundant information, leading to computational inefficiency.\n",
        "3. Optimal Choice: Choose the number of components that explain a high proportion of variance (e.g., 95%) without overfitting or computational burden.\n",
        "\n",
        "###[Q5.] How can PCA be used in feature selection, and what are the benefits of using it for this purpose?\n",
        "#####[Ans]\n",
        "PCA can serve as a dimensionality reduction tool by selecting principal components that explain the majority of variance:\n",
        "\n",
        "Benefits:\n",
        "- Reduces noise and redundancy.\n",
        "- Improves computational efficiency.\n",
        "- Helps avoid overfitting in high-dimensional datasets.\n",
        "\n",
        "However, PCA-transformed features are not directly interpretable as they are linear combinations of original features.\n",
        "\n",
        "###[Q6.] What are some common applications of PCA in data science and machine learning?\n",
        "#####[Ans]\n",
        "1. Dimensionality Reduction: Simplifying high-dimensional data for visualization or modeling.\n",
        "2. Data Visualization: Plotting data in 2D or 3D using the first few principal components.\n",
        "3. Noise Reduction: Filtering out less significant components that capture noise.\n",
        "4. Preprocessing for Machine Learning: Improving model performance by reducing input dimensions.\n",
        "5. Feature Extraction: Creating new features based on principal components.\n",
        "\n",
        "###[Q7.] What is the relationship between spread and variance in PCA?\n",
        "#####[Ans]\n",
        "- Spread refers to the distribution of data points in a given direction.\n",
        "- Variance is a measure of the spread along a specific axis or component.\n",
        "\n",
        "PCA seeks directions with maximum spread (variance) as these capture the most information about the data's structure.\n",
        "\n",
        "###[Q8.] How does PCA use the spread and variance of the data to identify principal components?\n",
        "#####[Ans]\n",
        "PCA computes the eigenvectors and eigenvalues of the covariance matrix:\n",
        "\n",
        "- Eigenvectors: Represent directions of maximum spread (principal components).\n",
        "- Eigenvalues: Represent the variance captured by each eigenvector.\n",
        "\n",
        "By projecting data onto components with high eigenvalues, PCA identifies directions that retain the most significant patterns in the data.\n",
        "\n",
        "###[Q9.] How does PCA handle data with high variance in some dimensions but low variance in others?\n",
        "#####[Ans]\n",
        "PCA automatically:\n",
        "\n",
        "1. Assigns higher importance to dimensions with high variance by selecting them as principal components.\n",
        "2. De-emphasizes dimensions with low variance, as they contribute less to the overall structure of the data.\n",
        "\n",
        "This prioritization is achieved through the eigenvalues, where dimensions with low eigenvalues (low variance) are discarded in dimensionality reduction."
      ],
      "metadata": {
        "id": "unfq9LlUw-oO"
      }
    }
  ]
}