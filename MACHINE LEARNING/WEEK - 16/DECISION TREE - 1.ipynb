{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###[Q1.] Describe the decision tree classifier algorithm and how it works to make predictions.\n",
        "\n",
        "#####[Ans]"
      ],
      "metadata": {
        "id": "wk76qEGQOrV3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Decision Tree Classifier builds a tree structure to classify data into categories. It works by:\n",
        "\n",
        "- Selecting the best feature for splitting the dataset at each node using criteria like Gini Impurity or Entropy (for Information Gain).\n",
        "\n",
        "- Splitting the data recursively into subsets until a stopping criterion is met, such as:\n",
        "A maximum tree depth.\n",
        "A minimum number of samples at a node.\n",
        "All samples in a node belong to a single class.\n",
        "\n",
        "- Making predictions by passing a sample through the tree from the root to a leaf node, where the label of the majority class is assigned."
      ],
      "metadata": {
        "id": "XQlVjCtYO-fM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###[Q2.] Provide a step-by-step explanation of the mathematical intuition behind decision tree classification.\n",
        "\n",
        "#####[Ans]"
      ],
      "metadata": {
        "id": "cXQ8Ned1PMyC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Entropy (H):\n",
        "Measures the impurity of a dataset:\n",
        "\n",
        "𝐻\n",
        "(\n",
        "𝑆\n",
        ")\n",
        "=\n",
        "−\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑐\n",
        "𝑝\n",
        "𝑖\n",
        "log\n",
        "⁡\n",
        "2\n",
        "(\n",
        "𝑝\n",
        "𝑖\n",
        ")\n",
        "H(S)=−\n",
        "i=1\n",
        "∑\n",
        "c\n",
        "​\n",
        " p\n",
        "i\n",
        "​\n",
        " log\n",
        "2\n",
        "​\n",
        " (p\n",
        "i\n",
        "​\n",
        " )\n",
        "where\n",
        "𝑝\n",
        "𝑖\n",
        "p\n",
        "i\n",
        "​\n",
        "  is the proportion of class\n",
        "𝑖\n",
        "i in the dataset\n",
        "𝑆\n",
        "S.\n",
        "\n",
        "2. Information Gain (IG):\n",
        "Measures the reduction in entropy achieved by splitting the dataset on a feature:\n",
        "\n",
        "𝐼\n",
        "𝐺\n",
        "(\n",
        "𝑆\n",
        ",\n",
        "𝐴\n",
        ")\n",
        "=\n",
        "𝐻\n",
        "(\n",
        "𝑆\n",
        ")\n",
        "−\n",
        "∑\n",
        "𝑣\n",
        "∈\n",
        "Values\n",
        "(\n",
        "𝐴\n",
        ")\n",
        "∣\n",
        "𝑆\n",
        "𝑣\n",
        "∣\n",
        "∣\n",
        "𝑆\n",
        "∣\n",
        "𝐻\n",
        "(\n",
        "𝑆\n",
        "𝑣\n",
        ")\n",
        "IG(S,A)=H(S)−\n",
        "v∈Values(A)\n",
        "∑\n",
        "​\n",
        "  \n",
        "∣S∣\n",
        "∣S\n",
        "v\n",
        "​\n",
        " ∣\n",
        "​\n",
        " H(S\n",
        "v\n",
        "​\n",
        " )\n",
        "where\n",
        "𝑆\n",
        "𝑣\n",
        "S\n",
        "v\n",
        "​\n",
        "  is the subset of\n",
        "𝑆\n",
        "S where feature\n",
        "𝐴\n",
        "A takes value\n",
        "𝑣\n",
        "v.\n",
        "\n",
        "Gini Impurity (G):\n",
        "An alternative to entropy:\n",
        "\n",
        "𝐺\n",
        "(\n",
        "𝑆\n",
        ")\n",
        "=\n",
        "1\n",
        "−\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑐\n",
        "𝑝\n",
        "𝑖\n",
        "2\n",
        "G(S)=1−\n",
        "i=1\n",
        "∑\n",
        "c\n",
        "​\n",
        " p\n",
        "i\n",
        "2\n",
        "​\n",
        "\n",
        "3. Splitting Criterion:\n",
        "The feature with the highest Information Gain or lowest Gini Impurity is chosen for splitting.\n",
        "\n",
        "4. Recursive Splitting:\n",
        "Repeats until the tree meets stopping criteria."
      ],
      "metadata": {
        "id": "wVAf_IW1PaJ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###[Q3.] Explain how a decision tree classifier can be used to solve a binary classification problem.\n",
        "\n",
        "#####[Ans]"
      ],
      "metadata": {
        "id": "bxugkdxyPpnJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In binary classification, a decision tree splits the dataset into two classe(e.g C1 & C2) by:\n",
        "\n",
        "1. Computing the Gini Impurity or Entropy for each feature.\n",
        "2. Selecting the feature and threshold that best separate the two classes.\n",
        "3. Recursively partitioning the data until all leaf nodes contain instances of only one class or meet other stopping criteria.\n",
        "4. Assigning the label of the majority class in the leaf node for predictions."
      ],
      "metadata": {
        "id": "xpO3mBIyP0Nm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###[Q4.] Discuss the geometric intuition behind decision tree classification and how it can be used to make predictions.\n",
        "\n",
        "#####[Ans]"
      ],
      "metadata": {
        "id": "mFG29gwCQUbb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- A decision tree divides the feature space into rectangular regions by making axis-aligned splits (e.g.,x1 > 5, x2 <= 3).\n",
        "- Each split corresponds to a hyperplane that partitions the space.\n",
        "- During prediction, a sample is placed in one of these regions, and the majority class in that region determines the prediction.\n"
      ],
      "metadata": {
        "id": "MNgw36jmQjVz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###[Q5.] Define the confusion matrix and describe how it can be used to evaluate the performance of a classification model.\n",
        "\n",
        "#####[Ans]\n",
        "\n",
        "A Confusion Matrix is a table that summarizes the performance of a classification model by comparing predicted labels to true labels:\n",
        "\n",
        "| Actual/Prediction | Positive | Negative |\n",
        "|-------------------|----------|----------|\n",
        "| Positive | TP | FN |\n",
        "| Negative | FP | TN |\n",
        "\n",
        "- True Positives (TP): Correctly predicted positives.\n",
        "- False Positives (FP): Incorrectly predicted positives.\n",
        "- True Negatives (TN): Correctly predicted negatives.\n",
        "- False Negatives (FN): Incorrectly predicted negatives."
      ],
      "metadata": {
        "id": "pQIGLEELQ2dB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###[Q6.] Provide an example of a confusion matrix and explain how precision, recall, and F1 score can be calculated from it.\n",
        "\n",
        "#####[Ans]\n",
        "\n",
        "| Actual/Predict | Positive | Negative |\n",
        "|----------------|----------|----------|\n",
        "| Positive | 50 | 10 |\n",
        "| Negative | 5 | 35 |\n",
        "\n",
        "- Precision = TP / (TP + FP) = 50 / (50 + 5) = 0.909\n",
        "- Recall = TP / (TP + FN) = 50 / (50 + 10) = 0.833\n",
        "- F1_Score = 2 * (precision * recall / precision + recall) = 2 * (0.909 * 0.833 / 0.909 + 0.833) = 0.87"
      ],
      "metadata": {
        "id": "F4ExQ6BuR-Wj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###[Q7.] Discuss the importance of choosing an appropriate evaluation metric for a classification problem and explain how this can be done.\n",
        "\n",
        "#####[Ans]\n",
        "\n",
        "Choosing the right evaluation metric ensures that the model aligns with the problem's requirements:\n",
        "\n",
        "- Use Accuracy when classes are balanced and all errors are equally costly.\n",
        "- Use Precision when the cost of false positives is high (e.g., spam email detection).\n",
        "- Use Recall when false negatives are critical (e.g., disease detection).\n",
        "- Use F1 Score when a balance between precision and recall is needed."
      ],
      "metadata": {
        "id": "XcOwS5WRT3C5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###[Q8.] Provide an example of a classification problem where precision is the most important metric, and explain why.\n",
        "\n",
        "#####[Ans]\n",
        "\n",
        "Example - Spam Email Detection.\n",
        "\n",
        "- Why Precision Matters: False positives (legitimate emails marked as spam) can result in important emails being missed.\n",
        "- The goal is to minimize false positives while maintaining a high precision."
      ],
      "metadata": {
        "id": "XuA_t9lqUFsP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###[Q9.] Provide an example of a classification problem where recall is the most important metric, and explain why.\n",
        "\n",
        "#####[Ans]\n",
        "\n",
        "Example - Disease Diagnosis.\n",
        "\n",
        "- Why Recall Matters: Missing a positive case (false negative) could delay treatment, leading to severe consequences.\n",
        "- The goal is to identify as many true positives as possible, even at the cost of some false positives."
      ],
      "metadata": {
        "id": "eOoUbTsMUYUY"
      }
    }
  ]
}