{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###[Q1.] What is Gradient Boosting Regression?\n",
        "#####[Ans]\n",
        "Gradient Boosting Regression is a machine learning technique that builds an ensemble of weak learners (typically decision trees) to predict a continuous target variable. The model is built iteratively, with each new learner trained to minimize the residual error of the previous learners. It uses gradient descent to optimize the loss function, which can be mean squared error or other regression-specific metrics."
      ],
      "metadata": {
        "id": "h47YlGncdIj6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###[Q2.] Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared.\n",
        "#####[Ans}"
      ],
      "metadata": {
        "id": "_DF4R8GbdOga"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6ryAT2uch9M",
        "outputId": "16912857-2ccd-48fb-af2e-7ccb40c7b430"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MEAN SQUARED ERROR : 1.5445\n",
            "R2 SCORE : 0.2016\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
        "y = np.array([1.2, 1.9, 3.1, 3.9, 5.1])\n",
        "\n",
        "n_trees = 10\n",
        "learning_rate = 0.1\n",
        "\n",
        "prediction = np.full_like(y, y.mean(), dtype='float')\n",
        "\n",
        "for _ in range(n_trees):\n",
        "   residuals = y - prediction\n",
        "   slope = np.sum((X.flatten() - X.mean()) * residuals) / np.sum((X.flatten() - X.mean())**2)\n",
        "   intercept = residuals.mean()\n",
        "\n",
        "   weak_prediction = slope * X.flatten() + intercept\n",
        "\n",
        "   prediction += learning_rate * weak_prediction\n",
        "\n",
        "mse = mean_squared_error(y, prediction)\n",
        "r2 = r2_score(y, prediction)\n",
        "\n",
        "print(f\"MEAN SQUARED ERROR : {mse:.4f}\")\n",
        "print(f\"R2 SCORE : {r2:.4f}\")\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###[Q3.] Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters.\n",
        "#####[Ans]"
      ],
      "metadata": {
        "id": "JUcPJbEyhW4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "X = np.random.rand(100, 1) * 10\n",
        "y = 3 * X.flatten() + np.random.randn(100) * 2\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "gbr = GradientBoostingRegressor(random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [1, 2, 3]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(estimator=gbr, param_grid=param_grid, scoring='neg_mean_squared_error', cv=3)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "print(f\"Mean Squared Error: {mse:.4f}\")\n",
        "print(f\"R2 Score: {r2:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XngrY5rmhgC7",
        "outputId": "c0ec490c-c05a-4a63-c761-2eec318c0d7a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 50}\n",
            "Mean Squared Error: 4.2173\n",
            "R2 Score: 0.9508\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###[Q4.] What is a Weak Learner in Gradient Boosting?\n",
        "#####[Ans]\n",
        "A weak learner is a simple model, typically a shallow decision tree, that performs slightly better than random guessing. In Gradient Boosting, weak learners are used to iteratively correct the errors made by previous learners."
      ],
      "metadata": {
        "id": "9kqxHxLCdkMO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###[Q5.] What is the Intuition Behind Gradient Boosting?\n",
        "#####[Ans]\n",
        "The core idea is to build models sequentially, where each new model aims to correct the residual errors of the combined model so far. Gradient descent is used to minimize a specified loss function, ensuring that each new model makes the ensemble more accurate."
      ],
      "metadata": {
        "id": "YyOq0lvWdquR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###[Q6.] How Does Gradient Boosting Build an Ensemble of Weak Learners?\n",
        "#####[Ans]\n",
        "1. Initialize predictions with a constant value (e.g., the mean for regression).\n",
        "2. Compute residuals based on the loss function.\n",
        "3. Train a weak learner to predict the residuals.\n",
        "4. Update the predictions using the output of the weak learner, scaled by the learning rate.\n",
        "5. Repeat the process for a specified number of iterations."
      ],
      "metadata": {
        "id": "iM9Fw0a6dyGI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###[Q7]. Steps to Build the Mathematical Intuition of Gradient Boosting:\n",
        "#####[Ans]\n",
        "\n",
        "1. Initialize a model with a constant prediction (e.g., mean or mode).\n",
        "2. Calculate Residuals: Measure the difference between the true values and current predictions.\n",
        "3. Fit Weak Learner: Train a weak model to predict the residuals.\n",
        "4. Update Predictions: Add the weak learnerâ€™s predictions to the current predictions, scaled by the learning rate.\n",
        "5. Optimize: Repeat the process to minimize the loss function iteratively."
      ],
      "metadata": {
        "id": "Ey4tC9Srd9YT"
      }
    }
  ]
}