{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###[Q1.] What is the KNN algorithm?\n",
        "#####[Ans]\n",
        "The K-Nearest Neighbors (KNN) algorithm is a simple, non-parametric, and lazy learning algorithm used for classification and regression tasks. It predicts the output for a new data point based on the majority class (in classification) or average value (in regression) of its K nearest neighbors in the training data, determined using a distance metric such as Euclidean distance.\n",
        "\n",
        "###[Q2.] How do you choose the value of K in KNN?\n",
        "#####[Ans]\n",
        "Choosing the value of K is critical to the performance of KNN:\n",
        "\n",
        "- Small K: Captures local patterns but may lead to overfitting and noise sensitivity.\n",
        "- Large K: Provides smoother predictions but may oversimplify and lose important local details.\n",
        "- Cross-Validation: Test different K values on validation data to find the optimal balance between bias and variance.\n",
        "- Odd Values: Often preferred for classification to avoid ties in majority voting.\n",
        "\n",
        "###[Q3.] What is the difference between KNN classifier and KNN regressor?\n",
        "#####[Ans]\n",
        "1. KNN Classifier:\n",
        "\n",
        "  - Used for categorical outputs (e.g., class labels).\n",
        "  - Assigns a class to the query point based on a majority vote of its K nearest neighbors.\n",
        "2. KNN Regressor:\n",
        "\n",
        "  - Used for continuous numerical outputs.\n",
        "  - Predicts the value of the query point as the average (or weighted average) of the K nearest neighbors' values.\n",
        "\n",
        "###[Q4.] How do you measure the performance of KNN?\n",
        "#####[Ans]\n",
        "- Classification Tasks:\n",
        "\n",
        "  - Accuracy\n",
        "  - Precision, Recall, F1-Score\n",
        "  - ROC-AUC for binary classification\n",
        "- Regression Tasks:\n",
        "\n",
        "  - Mean Squared Error (MSE)\n",
        "  - Mean Absolute Error (MAE)\n",
        "  - RÂ² Score (Coefficient of Determination)\n",
        "###[Q5.] What is the curse of dimensionality in KNN?\n",
        "#####[Ans]\n",
        "The curse of dimensionality refers to the phenomenon where the distance between data points becomes increasingly uniform as the number of dimensions (features) increases. This makes it difficult for KNN to effectively differentiate between neighbors, leading to poor performance.\n",
        "\n",
        "###[Q6.] How do you handle missing values in KNN?\n",
        "#####[Ans]\n",
        "1. Imputation Techniques:\n",
        "\n",
        "  - Replace missing values with mean, median, or mode for numerical features.\n",
        "  - Use the most frequent value for categorical features.\n",
        "2. KNN Imputation:\n",
        "\n",
        "  - Replace missing values based on the values of the K nearest neighbors.\n",
        "3. Feature Removal:\n",
        "\n",
        "  - Remove features with excessive missing values, though this can lead to information loss.\n",
        "\n",
        "###[Q7.] Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?\n",
        "#####[Ans]\n",
        "- KNN Classifier:\n",
        "\n",
        "  - Best suited for discrete, categorical classification tasks.\n",
        "  - Sensitive to imbalanced datasets and irrelevant features.\n",
        "- KNN Regressor:\n",
        "\n",
        "  - Suitable for continuous numerical predictions.\n",
        "  - Performs poorly when there is high noise in the data or when the target values vary significantly.\n",
        "\n",
        "###[Q8.] What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?\n",
        "#####[Ans]\n",
        "- Strengths:\n",
        "\n",
        "  - Simple and intuitive.\n",
        "  - No assumptions about the data distribution.\n",
        "  - Effective for smaller datasets with well-separated classes.\n",
        "- Weaknesses:\n",
        "\n",
        "  - Computationally expensive for large datasets.\n",
        "  - Sensitive to irrelevant or redundant features.\n",
        "  - Poor performance in high-dimensional spaces.\n",
        "\n",
        "###[Q9.] What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
        "#####[Ans]\n",
        "- Euclidean Distance:\n",
        "\n",
        "  - Measures the straight-line distance between two points.\n",
        "  - Formula:\n",
        "$$\n",
        "\\text{Distance} = \\sqrt{\\sum_{i=1}^n (x_i - y_i)^2}\n",
        "$$\n",
        "\n",
        "- Manhattan Distance:\n",
        "\n",
        "  - Measures the distance along the axes at right angles (like city block distance).\n",
        "  - Formula:\n",
        "$$\n",
        "\\text{Distance} = \\sum_{i=1}^n |x_i - y_i|\n",
        "$$\n",
        "\n",
        "- Key Difference:\n",
        "Euclidean distance is sensitive to feature scaling and captures direct distance, while Manhattan distance is robust to outliers and captures differences in axis-aligned directions.\n",
        "\n",
        "###[Q10.] What is the role of feature scaling in KNN?\n",
        "#####[Ans]\n",
        "Feature scaling ensures that all features contribute equally to distance calculations. Without scaling, features with larger ranges can dominate the distance metric, leading to biased predictions. Techniques like Min-Max Scaling or Standardization (Z-score normalization) are commonly used to standardize feature ranges.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mUMe5zI-U0XF"
      }
    }
  ]
}