{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###[Q1.] How does bagging reduce overfitting in decision trees?\n",
        "#####[Ans]\n",
        "Bagging reduces overfitting in decision trees by:\n",
        "\n",
        "- Creating diverse models: It trains multiple decision trees on different bootstrapped (resampled) subsets of the data, reducing sensitivity to noise in the dataset.\n",
        "- Aggregation: Combining the predictions of individual trees (e.g., majority voting for classification, averaging for regression) smooths out overfitted predictions from individual trees, leading to more generalizable results."
      ],
      "metadata": {
        "id": "ZB9Jf2_DvApC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###[Q2.] What are the advantages and disadvantages of using different types of base learners in bagging?\n",
        "#####[Ans]\n",
        "|Base Learner|\tAdvantages\t|Disadvantages|\n",
        "|------------| -------------|-------------|\n",
        "|Decision Trees\t|Captures complex patterns; performs well with bagging due to high variance.|\tProne to overfitting without bagging; computationally expensive for large trees.|\n",
        "|Linear Models|\tComputationally efficient; low bias.\t|May not benefit much from bagging as they have low variance.|\n",
        "|Weak Learners\t|Simple and fast; more diversity in ensemble.|\tMay require a larger ensemble to achieve good performance.|"
      ],
      "metadata": {
        "id": "QMG4eHf1vTdA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###[Q3.] How does the choice of base learner affect the bias-variance tradeoff in bagging?\n",
        "#####[Ans]\n",
        "- High-variance learners (e.g., decision trees): Bagging effectively reduces variance, leading to better generalization.\n",
        "- Low-variance learners (e.g., linear models): Bagging offers limited benefits, as these models are less sensitive to changes in training data.\n",
        "\n",
        "The effectiveness of bagging depends on the variance of the base learners; it is most useful when the base learner has high variance and relatively low bias.\n",
        "\n"
      ],
      "metadata": {
        "id": "XVU_WwxhwGnO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###[Q4.] Can bagging be used for both classification and regression tasks? How does it differ in each case?\n",
        "#####[Ans]\n",
        "Yes, bagging can be used for both tasks:\n",
        "\n",
        "- Classification: Aggregation is performed using majority voting across base learners.\n",
        "- Regression: Aggregation is performed by averaging predictions from base learners.\n",
        "\n",
        "Difference: In classification, the focus is on maximizing agreement between models, while in regression, the focus is on minimizing the mean squared error of predictions.\n",
        "\n"
      ],
      "metadata": {
        "id": "LavUeM9TwQON"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###[Q5.] What is the role of ensemble size in bagging? How many models should be included in the ensemble?\n",
        "#####[Ans]\n",
        "The ensemble size affects the performance and computational cost:\n",
        "\n",
        "- Smaller ensemble: May not sufficiently reduce variance.\n",
        "- Larger ensemble: Improves accuracy but increases computational cost, with diminishing returns after a certain size.\n",
        "\n",
        "Optimal Size: Typically, 100â€“500 models are sufficient for most applications, but the ideal size depends on the complexity of the dataset and the base learners."
      ],
      "metadata": {
        "id": "9k9laSHUwX9c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###[Q6.] Can you provide an example of a real-world application of bagging in machine learning?\n",
        "#####[Ans]"
      ],
      "metadata": {
        "id": "S2_PqNL5zIfD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaoA5kClu611",
        "outputId": "430d6a58-867e-47f1-b4c6-fcbacbb652df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACCURACY REPORT : 0.95\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      1.00      0.97       283\n",
            "           1       1.00      0.06      0.11        17\n",
            "\n",
            "    accuracy                           0.95       300\n",
            "   macro avg       0.97      0.53      0.54       300\n",
            "weighted avg       0.95      0.95      0.92       300\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "X, y = make_classification(\n",
        "    n_samples=1000, n_features=20, n_informative=10, n_redundant=5,\n",
        "    weights=[0.95, 0.05],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=1, class_weight=\"balanced\")\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"ACCURACY REPORT : {accuracy:.2f}\")\n",
        "print(classification_report(y_test, y_pred))"
      ]
    }
  ]
}