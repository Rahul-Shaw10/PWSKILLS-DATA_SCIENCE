{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a1bfe54-76e7-4418-b172-d1250c062c2d",
   "metadata": {},
   "source": [
    "### [Q1.] What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.\n",
    "##### [ANS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547dbc0d-a174-4483-b675-bab5dee4c12e",
   "metadata": {},
   "source": [
    "Min-Max scaling also known as normalization, is a data preprocessing technique used to rescale numerical features to a fixed range, typically between 0 and 1. This transformation preserves the original distribution of the data while ensuring that all the features have the same scale. Min-Max scaling is particularly useful when working with algorithms that require features to be on the same scale, such as neural networks, support vector machines, or k-nearest neighbors.\n",
    "\n",
    "Here's an example to illustrate Min-Max scaling:\n",
    "\n",
    "Suppose we have a dataset containing a feature representing the age of houses, with values ranging from 20 to 100 years. We want to scale this feature using Min-Max scaling to fit within the range 0 and 1.\n",
    "\n",
    "Original feature values:\n",
    "- House 1: Age = 20 years\n",
    "- House 2: Age = 50 years\n",
    "- House 3: Age = 100 years\n",
    "\n",
    "Using Min-Max scaling:\n",
    "\n",
    "- X<sub>min</sub> = 20 (minimum age)\n",
    "- X<sub>max</sub> = 100 (maximum age)\n",
    "- House 1: X<sub>scaled</sub> = $\\frac{20-20}{100-20}=0$\n",
    "- House 2: X<sub>scaled</sub> = $\\frac{50-20}{100-20}=0.333$\n",
    "- House 3: X<sub>scaled</sub> = $\\frac{100-20}{100-20}=1$\n",
    "\n",
    "Now, the scaled ages of the houses range from 0 to 1, preserving the relative differences between the original values while ensuring they are on the same scale. This scaled feature can then be used as input for machine learning models without biasing towards features with larger magnitudes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b9d4b5-fc48-4a90-b85d-89d43a55a523",
   "metadata": {},
   "source": [
    "### [Q2.] What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application.\n",
    "##### [ANS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b9d471-463a-4123-9bd7-2b6a15b4bd3f",
   "metadata": {},
   "source": [
    "\n",
    "The Unit Vector technique in feature scaling rescales numerical features so that each feature vector has a length of 1. This is achieved by dividing each feature vector by its Euclidean norm. Unlike Min-Max scaling, which scales features to a fixed range, Unit Vector scaling focuses on normalizing the direction of feature vectors rather than their magnitude. This technique is useful when the direction of the feature vectors is more important than their magnitude, such as in certain machine learning algorithms like cosine similarity calculations or in neural networks.\n",
    "\n",
    "Here's an example to illustrate the application of Unit Vector technique:\n",
    "\n",
    "Suppose we have a dataset with two numerical feature representing the height and weight of individuals:\n",
    "\n",
    "| Height (cm) | Weight (kg) |\n",
    "|-------------|-------------|\n",
    "|    170      |     65      |\n",
    "|    155      |     50      |\n",
    "|    180      |     70      |\n",
    "\n",
    "Using the Unit Vector Technique:\n",
    "- For the first row : Norm(x) = $\\sqrt{170^2 + 65^2}$\n",
    "- For the second row : Norm(x) = $\\sqrt{155^2 + 50^2}$\n",
    "- For the third row : Norm(x) = $\\sqrt{180^2 + 70^2}$\n",
    "\n",
    "- For the first row : Unit Vector(x) = $\\frac{170}{Norm(x)},\\frac{65}{Norm(x)}$\n",
    "- For the second row : Unit Vector(x) = $\\frac{155}{Norm(x)},\\frac{50}{Norm(x)}$\n",
    "- For the third row : Unit Vector(x) = $\\frac{180}{Norm(x)},\\frac{70}{Norm(x)}$\n",
    "\n",
    "The resulting unit vectors will have a length of 1, effectively normalizing the feature vectors. This example demonstrates how the Unit Vector technique rescales feature vectors to ensure they have the same scale, making them suitable for certain machine learning algorithms or analyses where the direction of the vectors is more important than their magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abb3068-168d-455c-906b-5479a3925685",
   "metadata": {},
   "source": [
    "### [Q3.] What is PCA (Principle Component Analysis), and how is it used in dimensionally reduction? Provide an example to illustrate its application.\n",
    "### [ANS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed00b09-cdc4-4f1a-b1eb-51f68c3937d8",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is a statistical technique used for dimensionality reduction in data analysis and machine learning. It works by transforming a dataset containing possibly correlated variables into a set of linearly uncorrelated variables called principal components. These components capture the maximum variance present in the original data, effectively reducing its dimensionality. PCA achieves this by finding the eigenvectors and eigenvalues of the covariance matrix of the original data and then selecting the top eigenvectors that explain the most variance. The data is then projected onto these selected principal components to obtain a lower-dimensional representation. PCA is commonly used for data visualization, noise reduction, and feature extraction tasks.\n",
    "\n",
    "Here's an example of PCA:\n",
    "\n",
    "Let’s say we have a data set of dimension 300 (n) × 50 (p). n represents the number of observations, and p represents the number of predictors. Since we have a large p = 50, there can be p(p-1)/2 scatter plots, i.e., more than 1000 plots possible to analyze the variable relationship. Wouldn’t it be a tedious job to perform exploratory analysis on this data?\n",
    "\n",
    "In this case, it would be a lucid approach to select a subset of p (p << 50) predictor which captures so much information, followed by plotting the observation in the resultant low-dimensional space.\n",
    "\n",
    "The image below shows the transformation of high-dimensional data (3 dimension) to low-dimensional data (2 dimension) using PCA. Not to forget, each resultant dimension is a linear combination of p features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5898f4-dc7d-47d3-9d19-bc27ea242b28",
   "metadata": {},
   "source": [
    "### [Q4.] What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.\n",
    "##### [ANS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856ff0b2-9580-4cce-8927-dfbd2131bff8",
   "metadata": {},
   "source": [
    "PCA and feature extraction are closely related concepts in data analysis and machine learning. PCA can be used as a feature extraction technique to transform high-dimensional data into a lower-dimensional representation while retaining as much relevant information as possible.\n",
    "\n",
    "In PCA, the principal components obtained represent new features that are linear combinations of the original features. These principal components capture the directions in which the data varies the most, allowing for a compact representation of the dataset. By selecting a subset of the principal components, we effectively perform feature extraction, reducing the dimensionality of the data while preserving its essential characteristics.\n",
    "\n",
    "For example, consider a dataset containing images represented as high-dimensional feature vectors, where each feature corresponds to a pixel intensity value. Applying PCA to this dataset can extract principal components that capture the most significant variations in the images. These principal components can be interpreted as features representing patterns or structures present in the images, such as edges, textures, or shapes. By selecting a subset of these principal components, we can effectively extract meaningful features from the images, reducing their dimensionality while preserving their discriminative information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b260121-5d47-4a58-89da-e7a615661504",
   "metadata": {},
   "source": [
    "### [Q5.]  You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data.\n",
    "##### [ANS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bf7849-7010-457c-b61d-1ccd52c9ca5e",
   "metadata": {},
   "source": [
    "To preprocess the data for building a recommendation system for a food delivery service using Min-Max scaling:\n",
    "1. Understand the dataset containing features like price, rating, and delivery time.\n",
    "2. Apply Min-Max scaling to each feature independently, rescaling values to a fixed range, typically [0,1].\n",
    "3. Use the formula x<sub>scaled</sub>= $$\\frac{x-min(x)}{max(x)-min(x)}$$ for each feature, where x is the original value, min(x) is the minimum value, and max(x) is the maximum value.\n",
    "4. The preprocessed data is now ready for building the recommendation system, with all features scaled to the same range for fair comparison and effective modeling.\n",
    "\n",
    "Here's an example:\n",
    "| Price ($) | Rating (out of 5) | Delivery Time (minutes) |\n",
    "|-----------|-------------------|------------------------|\n",
    "|    10     |        4.5        |           30           |\n",
    "|    20     |        3.8        |           45           |\n",
    "|    15     |        4.2        |           25           |\n",
    "\n",
    "Using Min-Max scaling:\n",
    "\n",
    "- min(x) = 10\n",
    "- max(x) = 20\n",
    "- x<sub>scaled</sub> = $\\frac{10-10}{20-10}=0$\n",
    "- x<sub>scaled</sub> = $\\frac{20-10}{20-10}=0$\n",
    "- x<sub>scaled</sub> = $\\frac{15-10}{20-10}=0$\n",
    "\n",
    "Similarly, apply Min-Max scaling to the Rating and Delivery Time features.\n",
    "After Min-Max scaling, all features will be scaled to the range [0, 1], making them suitable for building the recommendation system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc1d0bc-ddba-45a1-ab64-8c6ea68132ad",
   "metadata": {},
   "source": [
    "### [Q6.] You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset.\n",
    "##### [ANS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9147ac-ffec-485c-8bb6-4c2eba4ae05b",
   "metadata": {},
   "source": [
    "To reduce the dimensionality of the dataset for predicting stock prices using PCA, are as follows:\n",
    "\n",
    "Firstly, I would preprocess the dataset by removing any missing values and standardizing the features to ensure they have a mean of 0 and a standard deviation of 1. This step is crucial as PCA is sensitive to the scale of the features.\n",
    "\n",
    "Then, I would apply PCA to the standardized dataset. PCA works by identifying the directions (principal components) in which the data varies the most and projecting the original data onto these components while preserving as much variance as possible. By selecting a subset of these principal components that capture the most variance, I effectively reduce the dimensionality of the dataset.\n",
    "\n",
    "Next, I would determine the number of principal components to retain based on the amount of variance explained. This can be done by examining the explained variance ratio for each principal component. I would typically retain enough principal components to capture a significant portion of the variance in the data, such as 90% or 95%.\n",
    "\n",
    "Finally, I would transform the original dataset into the lower-dimensional space defined by the selected principal components. This reduced-dimensional dataset can then be used as input for building the predictive model to forecast stock prices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9ecdb0-5dfc-4dd4-98ca-79bae3784fdf",
   "metadata": {},
   "source": [
    "### [Q7.] For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1.\n",
    "##### [ANS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20355a90-effb-4c8a-9671-808e0db7607b",
   "metadata": {},
   "source": [
    "x = [1,5,10,15,20]\n",
    "\n",
    "min(x) = 1\n",
    "max(x) = 20\n",
    "\n",
    "For x = 1:\n",
    "x<sub>scaled</sub> = $\\frac{1-1}{20-1} = 0$\n",
    "\n",
    "For x = 5:\n",
    "x<sub>scaled</sub> = $\\frac{5-1}{20-1} = \\frac{4}{19}$\n",
    "\n",
    "For x = 10:\n",
    "x<sub>scaled</sub> = $\\frac{10-1}{20-1} = \\frac{9}{19}$\n",
    "\n",
    "For x = 15:\n",
    "x<sub>scaled</sub> = $\\frac{15-1}{20-1} = \\frac{14}{19}$\n",
    "\n",
    "For x = 20:\n",
    "x<sub>scaled</sub> = $\\frac{20-1}{20-1} = 1$\n",
    "\n",
    "Now, the Min-Max scaled values fot the dataset [1,5,10,15,20] in the range of -1 to 1 are:\n",
    "[-1 , $\\frac{4}{19} , \\frac{9}{19} , \\frac{14}{19}$ , 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841fa65b-5744-47b9-b1c8-41a2ca442510",
   "metadata": {},
   "source": [
    "### [Q8.] For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
    "##### [ANS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd924aba-b8bf-4d8b-978d-92f1ac09d9f6",
   "metadata": {},
   "source": [
    "| Height (cm) | Weight (kg) | Age (years) | Gender (0:Female, 1:Male) | Blood Pressure (mmHg) |\n",
    "|-------------|-------------|-------------|-----------------------------|------------------------|\n",
    "|    170      |     65      |     30      |              1              |           120          |\n",
    "|    160      |     55      |     35      |              0              |           130          |\n",
    "|    180      |     70      |     40      |              1              |           125          |\n",
    "|    165      |     60      |     28      |              0              |           135          |\n",
    "|    175      |     75      |     45      |              1              |           130          |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1f7682a-5fed-409a-a1de-5f438c1fa25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of principal components retained: 2\n",
      "Reduced-dimensional data:\n",
      "[[-0.53949867  1.88841745]\n",
      " [ 2.20221657 -0.14352644]\n",
      " [-1.92431413  0.02904393]\n",
      " [ 2.20287387 -0.4953814 ]\n",
      " [-1.94127764 -1.27855354]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sample dataset\n",
    "data = [[170, 65, 30, 1, 120],\n",
    "        [160, 55, 35, 0, 130],\n",
    "        [180, 70, 40, 1, 125],\n",
    "        [165, 60, 28, 0, 135],\n",
    "        [175, 75, 45, 1, 130]]\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA()\n",
    "pca.fit(scaled_data)\n",
    "\n",
    "# Determine the number of principal components to retain\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "cumulative_variance_ratio = explained_variance_ratio.cumsum()\n",
    "num_components = (cumulative_variance_ratio < 0.90).sum() + 1\n",
    "\n",
    "# Transform the data\n",
    "pca = PCA(n_components=num_components)\n",
    "reduced_data = pca.fit_transform(scaled_data)\n",
    "\n",
    "print(\"Number of principal components retained:\", num_components)\n",
    "print(\"Reduced-dimensional data:\")\n",
    "print(reduced_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
