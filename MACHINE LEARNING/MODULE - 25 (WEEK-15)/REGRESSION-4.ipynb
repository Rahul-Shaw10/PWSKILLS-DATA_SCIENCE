{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "640a7801-0042-4f52-a7dd-92e2aa4aa217",
   "metadata": {},
   "source": [
    "### [Q1.] What is Lasso Regression, and how does it differ from other regression techniques?\n",
    "##### [ANS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a199fffe-978c-47e1-8a33-0f641dcbe38a",
   "metadata": {},
   "source": [
    "Lasso Regression is type of linear regression that adds a penalty term to the ordinary least squares (OLS) regression objective function. THis penalty term, also known as L1 regularization, is proportional to the sum of the absolute valyues of the coefficients. Unlike other regression techniques, Lasso Regression has the property of setting some Coefficients exactly to zero, effectively performing variable selection and producing sparse models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b41da7f-0b99-402b-9ea6-1795955e6999",
   "metadata": {},
   "source": [
    "### [Q2.] What is the main advantage of using Lasso Regression in feature selection?\n",
    "##### [ANS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6b4e58-550e-4553-9ddc-c35b2be3c85e",
   "metadata": {},
   "source": [
    "The main advantage of using Lasso Regression in feature selection is its ability to automatically select a subset of the most relevant predictors while setting the coefficients of irrelevant predictors to zero. This feature makes Lasso Regression particularly useful when dealing with high-dimensinal datasets containing many features, as it helps in identifying the most important predictors and simlifying the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b62de17-2c64-4152-8d45-df1035608749",
   "metadata": {},
   "source": [
    "### [Q3.] How do you interpret the coefficients of a Lasso Regression model?\n",
    "##### [ANS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776df8e6-d521-4e0e-9ed2-163510dd4362",
   "metadata": {},
   "source": [
    "The interpretation of coefficients in a Lasso Regression model is similar to that in ordinary least squares regression. Each coefficient represents the change in the dependent variable associated witha one-unit change in the corresponding independent variable, holding all other variables constant. However, because Lasso Regression tends to shrink some coefficients to zero, the coefficients that remain non zero indicates the most influential predictors in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303d5135-020e-4a9e-9b3f-ca80cbabe863",
   "metadata": {},
   "source": [
    "### [Q4.] What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?\n",
    "##### [ANS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeadb36b-8abe-444c-913a-9dd9b4efd363",
   "metadata": {},
   "source": [
    "The tuning parameter in Lasso Regression is often denoted as lambda. The value of lambda controls the strength of the penalty term added to the objective fubnction. A higher value of lambda results in stronger regularization, leading to more coefficients being set to zero and a sparser model. Conversely, a lower value of lambda reduces the regularization effect, allowing more coefficients to remain non zero and potentially leading to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e80bae-ea02-4400-8faf-7ae0ab267bb4",
   "metadata": {},
   "source": [
    "### [Q5.] Can Lasso Regression be used for non-linear regression problems? If yes, how?\n",
    "##### [ANS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b048342e-385c-45da-b77d-706f9f54e2f4",
   "metadata": {},
   "source": [
    "Lasso Regression can be used for non-linear regression problems by incorporating non-linear transformations of the original features into the model. For example, polynomial features or interaction terms can be added to the regression model to capture non-linear relationships between the predictors and the response variable. Additionally, Lasso Regression can be combined with kernel methods to handle non-linearities in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fabedd0-aa7d-4619-8d26-dd046d49c331",
   "metadata": {},
   "source": [
    "### [Q6.] What is the difference between Ridge Regression and Lasso Regression?\n",
    "##### [ANS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cd58d2-bd71-4a06-8c7b-abf4f16b21a3",
   "metadata": {},
   "source": [
    "The main difference between Ridge Regression and Lasso Regression lies in the type of penalty term used for regularization. Ridge Regression adds a penalty term proportional to the sum of the squared coefficients (L2 regularization), whereas Lasso Regression adds a penalty term proportional to the sum of the absolute values of the coefficients (L1 regularization). As a result, Ridge Regression tends to shrink coefficients towards zero without necessarily setting them exactly to zero, while Lasso Regression can set some coefficients exactly to zero, effectively performing variable selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf7b895-7fc2-4acb-ab12-fe180f1a0aa8",
   "metadata": {},
   "source": [
    "### [Q7.] Can Lasso Regression handle multicollinearity in the input features? If yes, how?\n",
    "##### [ANS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fa688b-86c9-43f9-8789-eaf61daff7b7",
   "metadata": {},
   "source": [
    "Yes, Lasso Regression can handle multicollinearity in the input features. The penalty term added to the objective function encourages sparsity in the coefficient estimates, effectively selecting a subset of the most relevant predictors while setting the coefficients of correlated or redundant predictors to zero. This helps in reducing the impact of multicollinearity on the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fddf85-63f1-46ef-997f-b953568b8fb8",
   "metadata": {},
   "source": [
    "### [Q8.] How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?\n",
    "##### [ANS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f831985d-ceae-48fa-9c5a-9d2c24862d21",
   "metadata": {},
   "source": [
    "The optimal value of the regularization parameter (lambda) in Lasso Regression is typically chosen through techniques such as cross-validation. Cross-validation involves splitting the dataset into training and validation sets multiple times, fitting Lasso Regression models with different values of lambda on the training set, and selecting the value of lambda that results in the best performance on the validation set. Regularization parameter tuning methods, such as grid search or random search, can also be used to efficiently search for the optimal lambda value over a predefined range."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
