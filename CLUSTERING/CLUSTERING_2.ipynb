{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Q1. What is hierarchical clustering, and how is it different from other clustering techniques?\n",
        "#####[Ans]\n",
        "### Definition:\n",
        "Hierarchical clustering is a clustering technique that builds a hierarchy of clusters. It either starts with individual data points and merges them iteratively (agglomerative) or starts with all data points in one cluster and splits them iteratively (divisive).\n",
        "\n",
        "### Key Differences from Other Techniques:\n",
        "1. **Structure**:\n",
        "   - Hierarchical clustering creates a tree-like structure (dendrogram), while methods like K-Means directly partition the data into clusters.\n",
        "   \n",
        "2. **Number of Clusters**:\n",
        "   - Does not require the number of clusters (K) to be specified beforehand, unlike K-Means.\n",
        "\n",
        "3. **Flexibility**:\n",
        "   - Can produce clusters at different levels of granularity, depending on where the dendrogram is cut.\n",
        "\n",
        "---\n",
        "\n",
        "## Q2. What are the two main types of hierarchical clustering algorithms? Describe each in brief.\n",
        "#####[Ans]\n",
        "### 1. Agglomerative Clustering:\n",
        "   - A \"bottom-up\" approach.\n",
        "   - Starts with each data point as its own cluster and merges the closest clusters iteratively until all points belong to one cluster.\n",
        "   - Commonly used due to simplicity and efficiency.\n",
        "\n",
        "### 2. Divisive Clustering:\n",
        "   - A \"top-down\" approach.\n",
        "   - Starts with all data points in a single cluster and splits them iteratively into smaller clusters.\n",
        "   - Less commonly used due to higher computational complexity.\n",
        "\n",
        "---\n",
        "\n",
        "## Q3. How do you determine the distance between two clusters in hierarchical clustering, and what are the common distance metrics used?\n",
        "#####[Ans]\n",
        "### Distance Between Clusters:\n",
        "1. **Single Linkage**:\n",
        "   - Distance between the closest points of two clusters.\n",
        "\n",
        "2. **Complete Linkage**:\n",
        "   - Distance between the farthest points of two clusters.\n",
        "\n",
        "3. **Average Linkage**:\n",
        "   - Average distance between all pairs of points in two clusters.\n",
        "\n",
        "4. **Centroid Linkage**:\n",
        "   - Distance between the centroids of two clusters.\n",
        "\n",
        "5. **Ward’s Method**:\n",
        "   - Minimizes the variance within clusters by merging clusters that lead to the smallest increase in total variance.\n",
        "\n",
        "### Common Distance Metrics:\n",
        "- **Euclidean Distance**\n",
        "- **Manhattan Distance**\n",
        "- **Cosine Similarity**\n",
        "- **Hamming Distance** (for categorical data)\n",
        "\n",
        "---\n",
        "\n",
        "## Q4. How do you determine the optimal number of clusters in hierarchical clustering, and what are some common methods used for this purpose?\n",
        "#####[Ans]\n",
        "### Methods:\n",
        "1. **Dendrogram**:\n",
        "   - Cut the dendrogram at the level where the vertical distance between successive merges is largest.\n",
        "\n",
        "2. **Silhouette Analysis**:\n",
        "   - Evaluates how well each point lies within its cluster. The optimal number of clusters maximizes the silhouette score.\n",
        "\n",
        "3. **Elbow Method**:\n",
        "   - Similar to K-Means, plots the within-cluster sum of squares (WCSS) and identifies the \"elbow\" point.\n",
        "\n",
        "---\n",
        "\n",
        "## Q5. What are dendrograms in hierarchical clustering, and how are they useful in analyzing the results?\n",
        "#####[Ans]\n",
        "### Definition:\n",
        "A dendrogram is a tree-like diagram that illustrates the arrangement of clusters formed by hierarchical clustering.\n",
        "\n",
        "### Usefulness:\n",
        "1. **Visual Representation**:\n",
        "   - Shows how clusters are merged or split at each step.\n",
        "   \n",
        "2. **Identifying Optimal Clusters**:\n",
        "   - Helps determine the number of clusters by cutting the tree at an appropriate height.\n",
        "   \n",
        "3. **Interpreting Relationships**:\n",
        "   - Reveals relationships and similarities between data points and clusters.\n",
        "\n",
        "---\n",
        "\n",
        "## Q6. Can hierarchical clustering be used for both numerical and categorical data? If yes, how are the distance metrics different for each type of data?\n",
        "#####[Ans]\n",
        "### Numerical Data:\n",
        "- Distance metrics: **Euclidean**, **Manhattan**, **Minkowski**, etc.\n",
        "\n",
        "### Categorical Data:\n",
        "- Distance metrics:\n",
        "  1. **Hamming Distance**: Measures the proportion of mismatched attributes.\n",
        "  2. **Jaccard Similarity**: Measures similarity based on shared attributes.\n",
        "\n",
        "### Mixed Data:\n",
        "- Combine numerical and categorical distances using methods like **Gower’s Distance**.\n",
        "\n",
        "---\n",
        "\n",
        "## Q7. How can you use hierarchical clustering to identify outliers or anomalies in your data?\n",
        "#####[Ans]\n",
        "### Steps:\n",
        "1. Perform hierarchical clustering and build a dendrogram.\n",
        "2. Identify points that form singleton clusters (clusters with a single point).\n",
        "3. Observe large distances in the dendrogram where individual points are linked to clusters.\n",
        "\n",
        "### Insights:\n",
        "- Outliers are typically far away from other clusters or linked at the top of the dendrogram.\n",
        "- Provides a systematic way to detect anomalies without predefined thresholds.\n",
        "\n"
      ],
      "metadata": {
        "id": "tqPo4tyx_8rG"
      }
    }
  ]
}