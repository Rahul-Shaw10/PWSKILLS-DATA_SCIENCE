{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a85b8970-8b6f-4bcf-a80f-a1beecedb8b6",
   "metadata": {},
   "source": [
    "### [Q1.] Define overfitting and underfitting in machine learning. What are the consequences of each, and how can they be mitigated?\n",
    "##### [Ans]\n",
    "\n",
    "**OVERFITTING :**<br>\n",
    "\n",
    "*DEFINATION* :- Overfitting occurs when a machine learning model learns the training data too well, capturing noise and random fluctuations rather than the underlying patterns. As a result, the model performs well on the training data but fails to generalize to new, unseen data.\n",
    "\n",
    "*CONSEQUENCES* :- The model may have poor performance on new data, leading to decreased accuracy and reliability. Overfit models tend to be overly complex and may not generalize well beyond the specific examples in the training set.\n",
    "<br>\n",
    "\n",
    "*MITIGATION STRATEGIES* :-<br>\n",
    "- Feature Selection: Remove irrelevant or redundant features to reduce the model's complexity\n",
    "- Data Augmentation: Increase the size of the training set by generating new examples through transformations like rotation, cropping, or flipping.\n",
    "_______\n",
    "\n",
    "**UNDERFITTING :**<br>\n",
    "\n",
    "*DEFINATION* :- Underfitting happens when a model is too simple and fails to capture the underlying patterns in the training data. The model lacks the capacity to represent the complexity of the true relationship between input and output.\n",
    "\n",
    "*CONSEQUENCES* :- The model may have poor performance on new data, leading to decreased accuracy and reliability. Overfit models tend to be overly complex and may not generalize well beyond the specific examples in the training set.\n",
    "\n",
    "*MITIGATION STRATEGIES* :-<br>\n",
    "- Feature Engineering: Introduce more relevant features that better represent the relationships within the data.\n",
    "- Ensemble Methods: Combine multiple weak models to create a stronger, more complex model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbeb0c4f-1642-4e6c-a4aa-7d9d9633a8fb",
   "metadata": {},
   "source": [
    "### [Q2.] How can we reduce overfitting? Explain in brief.\n",
    "##### [Ans]\n",
    "\n",
    "To reduce overfitting in machine learning we can do this as follows :<br>\n",
    "\n",
    "- **Regularization :** Add penalties for complex models (L1, L2 regularization).<br>\n",
    "- **Cross-Validation :** Evaluate performance on validation sets during training.<br>\n",
    "- **Feature Selection :** Remove irrelevant or redundant features.<br>\n",
    "- **Early Stopping :** Halt training when validation performance degrades.<br>\n",
    "- **Data Augmentation :** Increase training set divbersity with transformed examples.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928c8ed7-e2d5-439c-81b1-29fb38e93170",
   "metadata": {},
   "source": [
    "### [Q3.] Explain underfitting. List scenarios where underfitting can occur in ML.\n",
    "##### [Ans]\n",
    "\n",
    "Underfitting happens when a model is too simple and fails to capture the underlying patterns in the training data. The model lacks the capacity to represent the complexity of the true relationship between input and output.\n",
    "\n",
    "Underfitting in ML can occur when :\n",
    "- The model is too simple for complex data.\n",
    "- Relevant features are missing.\n",
    "- Outliers are not handled properly.\n",
    "- Excessive regularization limits model complexity.\n",
    "- The dataset is too small for the chosen model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63144b3-e792-4e1c-b571-73dd673e69b6",
   "metadata": {},
   "source": [
    "### [Q4.]Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and variance, and how do they affect model performance?\n",
    "##### [Ans]\n",
    "\n",
    "**DEFINATION :-** The bias-variance tradeoff is a fundamental concept in machine learning that involves balancing two sources of error, namely bias and variance, to achieve optimal model performance.\n",
    "\n",
    "**RELATIONSHIP :-** The relationship between bias and variance is inversely proportional. As you decrease bias, variance tends to increse, and vice-versa.\n",
    "\n",
    "**IMPACT ON MODEL PERFORMANCE :**\n",
    "High bias leads to underfitting and high variance leads to overfitting to the model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9804ad66-0170-4f38-884f-d10d8d2bc9fc",
   "metadata": {},
   "source": [
    "### [Q5.] Discuss some common methods for detecting overfitting and underfitting in machine learning models. How can you determine whether your model is overfitting or underfitting?\n",
    "##### [Ans]\n",
    "\n",
    "To detect overfitting and underfitting in machine learning model we can do as follows :\n",
    "1. **Using training and validation curves :** Plotting the training and validation curves of a model can help detect over-fitting and under-fitting. If the training error is much lower than the validation error, it indicates that the model is over-fitting. If both the training and validation errors are high, it indicates that the model is underfitting.\n",
    "2. **Using learning curves :** Learning curves show how the model's performance improves as the size of the training data increases. If the learning curve plateaus, it indicates that the model is unable to learn from additional training data, and the model may be under-fitting.\n",
    "\n",
    "**To determine whether a model is overfitting or underfitting,** we can use the above methods to analyze the model's performance. If the training error is low, but the validation error is high, it indicates that the model is overfitting. If both the training and validation errors are high, it indicates thet the model is underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b467f3b6-9a3e-49a6-8493-f2f90b74913a",
   "metadata": {},
   "source": [
    "### [Q6.] Compare and contrast bias and variance in machine learning. What are some examples of high bias and high variance models, and how do they differ in terms of their performance?\n",
    "##### [Ans]\n",
    "\n",
    "Bias and variance are two sources of error in machine learning models. Bias refers to the difference between the expected output and the true output of the model, while variance refers to the variability of the model's output for different inputs.\n",
    "\n",
    "**High bias** models are typically too simple and unable to capture the underlying patterns in the data. They tend to under-fit the data, leading to high training and test errors. High bias models have low complexity and often have fewer parameters than the data requires.\n",
    "\n",
    "**Examples of high bias models** include linear regression models, which assume that the relationship between the inputs and the output is linear, even when it is not.\n",
    "\n",
    "**High variance models** are typically too complex and able to fit the training data too closely, including noise in the data. They tend to over-fit the data, leading to low training error but high test error. High variance models have high complexity and often have more parameters than necessary.\n",
    "\n",
    "**Examples of high variance models** include decision trees with deep and complex branches, which can fit the training data too closely.\n",
    "\n",
    "**The main difference between high bias and high variance models** is their performance. High bias models perform poorly on both training and test data, while high variance models perform well on training data but poorly on test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e40a06c-f88c-4e7c-8ab3-9774a5dfcda5",
   "metadata": {},
   "source": [
    "### [Q7.] What is regularization in machine learning, and how can it be used to prevent overfitting? Describe some common regularization techniques and how they work.\n",
    "##### [Ans]\n",
    "\n",
    "**DEFINATION :-** Regularization is a technique used in machine learning to prevent overfitting and improve the generalization of a model. Overfitting occurs when a model learns the training data too well, capturing noise and complex patterns that don't generalize to new, unseen data. Regularization introduces a penalty term to the model's cost function, discouraging the learning of overly complex relationships within the data.\n",
    "\n",
    "**COMMON REGULARIZATION TECHNIQUES :-**\n",
    "- **REGULARIZATION :** Regularization is used to prevent overfitting by adding penalties for complex models, promoting better generalization to new data.<br>\n",
    "- **ELASTIC NET REGULARIZATION :** Elastic Net combines both L1 and L2 penalties, providing a balance between sparsity and regularization, especially useful with correlated features.<br>\n",
    "- **DROPOUT :** Dropout randomly drops neurons during training, reducing over-reliance on specific features and improving the robustness of neural networks.<br>\n",
    "- **EARLY STOPPING :** Early stopping halts the training process when the model's performance on a validation set starts deteriorating, preventing overfitting to the training data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
