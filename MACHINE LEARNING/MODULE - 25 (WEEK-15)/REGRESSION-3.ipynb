{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acea7a90-785b-42af-a0c0-7792cea0f71b",
   "metadata": {},
   "source": [
    "### [Q1.] What is Ridge Regression, and how does it differ from ordinary least squares regression?\n",
    "##### [ANS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f56a9f-c1d9-4271-8ba6-2f158390d8fc",
   "metadata": {},
   "source": [
    "Ridge Regression is a type of linear regression technique that adds a penalty term to the ordinary least squares (OLS) regression objective function. This penalty term, also known as L2 regularization, is proportional to the sum of the squared coefficients of the regression model. The primary difference between Ridge Regression and ordinary least squares regression is that Ridge Regression seeks to minimize the residual sum of squares (RSS) while also penalizing large coefficients, thus preventing overfitting and improving the model's generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ad4310-2656-47a9-879d-3074f5e70feb",
   "metadata": {},
   "source": [
    "### [Q2.] What are the assumptions of Ridge Regression?\n",
    "##### [ANS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb6e19a-f15e-4a84-9c92-9302597d00e6",
   "metadata": {},
   "source": [
    "The assuumptions of Ridge Regression are as folloes :\n",
    "- Linearity - The relationship between the independent variables and the dependent variable is assumed to be linear.\n",
    "- Independence - The observations are assumed to be independent of each other.\n",
    "- Homoscedasticity - The variance of the errors is assumed to be constant across all levels of the independent variables.\n",
    "- Normality - The errors are assumed to be normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fdf32e-e197-4b34-b960-f2552a1d81cf",
   "metadata": {},
   "source": [
    "### [Q3.] How do you select the value of the tuning parameter (lambda) in Ridge Regression?\n",
    "##### [ANS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ff0001-3eea-4d63-a3de-09f1a271ca5c",
   "metadata": {},
   "source": [
    "The value of the tuning parameter (lambda) in Ridge Regression is typically selected through techniques such as cross-validation. Cross-validation involves splitting the dataset into training and validation sets multiple times, fitting Ridge Regression models with different values of lambda on the training set, and selecting the value of lambda that results in the best performance on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5076ad-0a7e-402f-bae0-22d0976218f8",
   "metadata": {},
   "source": [
    "### [Q4.] Can Ridge Regression be used for feature selection? If yes, how?\n",
    "##### [ANS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d590c0a-44c7-41ff-ab58-112b91721760",
   "metadata": {},
   "source": [
    "Yes, Ridge Regression can be uused for feature selection. Although it does not set coefficients excatly to zero like Lasso Regression, Ridge Regression tends to shrink coefficients towards zero, effectively reducing the impact of less relevant features. Features with similar coefficients after Ridge regulaization can be considered less influential in the model, effectively achieving a form of feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716d969e-918b-4b81-a293-0ff59c0d8896",
   "metadata": {},
   "source": [
    "### [Q5.] How does the Ridge Regression model perform in the presence of multicollinearity?\n",
    "##### [ANS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eec12ab-3cb0-4f74-a8bf-22614984f25f",
   "metadata": {},
   "source": [
    "Ridge Regression performs well in the prsence of muulticollinearity. The penalty term added to the objective fnction penalizes large coefficients, which helps stabilize the estimates of the regression coefficients, even when the independent variables are highly correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6a74f9-be94-481d-92d6-6fbe3509380b",
   "metadata": {},
   "source": [
    "### [Q6.] Can Ridge Regression handle both categorical and continuous independent variables?\n",
    "##### [ANS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748101eb-69b9-44b2-98c5-fe7d52e8c649",
   "metadata": {},
   "source": [
    "Yes, Ridge Regression can handle both categorical and continous independent variables. Categorical variables need to be properly encoded before being incluuded in the regression model, such as through one-hot encoding or variable encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a0d83e-901e-4c14-bbfe-e6f78afd37df",
   "metadata": {},
   "source": [
    "### [Q7.] How do you interpret the coefficients of Ridge Regression?\n",
    "##### [ANS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daeb83cc-4aeb-4cd2-8426-409440a887a6",
   "metadata": {},
   "source": [
    "The interpretation of coefficients in Ridge Regression is similar to that in ordinary least squares regression. Each coeffiecient represents the change in the dependent variable associated with one-unit change in the corresponding independent variable, holding all other variables constant. However, because Ridge Regression penalizes large coefficients, the magnitude of the coefficients may be smaller comapred to OLS regression, and their interpretation should consider the regularization effect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c852dd51-3384-4821-a04c-d08fe7ffd841",
   "metadata": {},
   "source": [
    "### [Q8.] Can Ridge Regression be used for time-series data analysis? If yes, how?\n",
    "##### [ANS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa08cf3f-9a2b-4b43-a761-499925f0bcd7",
   "metadata": {},
   "source": [
    "Yes, Ridge Regression can be used for time-series data analysis. In time-series anlysis, Ridge Regression can be applied similarly to cross-sectional data, with the additional consideration of temporal dependencies between observations. Time-dependent features, such as lagged variables, can be included in the regression model alongside other independent variables. Ridge Regression helps mitigate overfitting and stabilizes coefficient estimates, making it suitable for time-series analysis, especially when multicollinearity is present."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
