{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###[Q1.] What is the main difference between the Euclidean distance metric and the Manhattan distance metric in KNN? How might this difference affect the performance of a KNN classifier or regressor?\n",
        "#####[Ans]\n",
        "- Euclidean Distance measures the straight-line (or \"as the crow flies\") distance between two points in space. It is more sensitive to large differences in feature values.\n",
        "Formula:\n",
        "$$\n",
        "\\text{Distance} = \\sqrt{\\sum_{i=1}^n (x_i - y_i)^2}\n",
        "$$\n",
        "\n",
        "- Manhattan Distance measures the distance between two points along axes at right angles, following a grid-like path. It is less sensitive to outliers.\n",
        "Formula:\n",
        "$$\n",
        "\\text{Distance} = \\sum_{i=1}^n |x_i - y_i|\n",
        "$$\n",
        "- Impact on Performance:\n",
        "\n",
        "  - Euclidean distance works better for problems where all features contribute equally to the target value.\n",
        "  - Manhattan distance is preferred when dealing with high-dimensional data or where features have varying scales.\n",
        "\n",
        "###[Q2.] How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be used to determine the optimal k value?\n",
        "#####[Ans]\n",
        "- The value of ùëò controls the number of neighbors considered during prediction.\n",
        "- Techniques to find optimal ùëò:\n",
        "  1. Cross-Validation: Test different values of ùëò using validation datasets to determine the value that minimizes the error rate.\n",
        "  2. Grid Search: Automate the search over a range of ùëò values to find the one yielding the best performance.\n",
        "  3. Elbow Method: Plot the error rate against ùëò and choose the ùëò where the error stabilizes or is minimized.\n",
        "- Lower ùëò might result in high variance, while higher ùëò can lead to high bias.\n",
        "\n",
        "###[Q3.] How does the choice of distance metric affect the performance of a KNN classifier or regressor? In what situations might you choose one distance metric over the other?\n",
        "#####[Ans]\n",
        "- Impact of Distance Metric:\n",
        "\n",
        "  - The distance metric determines how neighbors are selected.\n",
        "  - Euclidean distance performs better when feature importance is uniform and differences in magnitude matter.\n",
        "  - Manhattan distance is better suited for high-dimensional or sparse data.\n",
        "- Choosing Distance Metric:\n",
        "\n",
        "  - Use Euclidean distance for continuous data and low dimensions.\n",
        "  - Use Manhattan distance for categorical data or when features vary widely in scale.\n",
        "\n",
        "###[Q4.] What are some common hyperparameters in KNN classifiers and regressors, and how do they affect the performance of the model? How might you go about tuning these hyperparameters to improve model performance?\n",
        "#####[Ans]\n",
        "- Common Hyperparameters:\n",
        "\n",
        "  1. k (Number of Neighbors): Controls the size of the neighborhood considered.\n",
        "Lower ùëò: Sensitive to noise, low bias, high variance.\n",
        "    - Higher ùëò: Smoother decision boundaries, high bias, low variance.\n",
        "    - Distance Metric: Defines how neighbors are calculated (e.g., Euclidean, Manhattan, Minkowski).\n",
        "  2. Weighting Scheme: Uniform weighting or distance-weighted neighbors.\n",
        "  3. Distance-weighted can improve performance by giving closer neighbors more influence.\n",
        "- Hyperparameter Tuning:\n",
        "\n",
        "  - Use grid search or random search to explore different combinations of ùëò, distance metrics, and weighting schemes.\n",
        "  - Employ cross-validation to evaluate the performance of different configurations.\n",
        "\n",
        "###[Q5.] How does the size of the training set affect the performance of a KNN classifier or regressor? What techniques can be used to optimize the size of the training set?\n",
        "#####[Ans]\n",
        "- Effect of Training Set Size:\n",
        "\n",
        "  - Larger training sets improve accuracy by providing more data for the algorithm to find patterns.\n",
        "  - Smaller training sets can lead to underfitting and reduced generalization\n",
        "  \n",
        "- Techniques to Optimize Training Set Size:\n",
        "\n",
        "  1. Use sampling techniques to select representative subsets.\n",
        "  2. Apply dimensionality reduction (e.g., PCA) to reduce computation without sacrificing performance.\n",
        "  3. Ensure stratification to maintain class distributions in classification problems.\n",
        "\n",
        "###[Q6.] What are some potential drawbacks of using KNN as a classifier or regressor? How might you overcome these drawbacks to improve the performance of the model?\n",
        "#####[Ans]\n",
        "- Drawbacks:\n",
        "\n",
        "  1. High Computation Cost: KNN is computationally expensive for large datasets as it requires storing and searching the entire training set.\n",
        "  2. Curse of Dimensionality: Performance degrades with high-dimensional data because distance metrics become less meaningful.\n",
        "  3. Sensitive to Noise: KNN is prone to errors if the dataset contains noisy points.\n",
        "  4. Feature Scaling: Sensitive to differences in scale, requiring careful preprocessing.\n",
        "- Solutions:\n",
        "\n",
        "  1. Use KD-Trees or Ball Trees for faster neighbor searches.\n",
        "  2. Apply feature scaling (e.g., standardization or normalization).\n",
        "  3. Use dimensionality reduction to mitigate the curse of dimensionality.\n",
        "  4. Employ weighted KNN to reduce the impact of noisy or distant points."
      ],
      "metadata": {
        "id": "G_9zEqedX3iv"
      }
    }
  ]
}