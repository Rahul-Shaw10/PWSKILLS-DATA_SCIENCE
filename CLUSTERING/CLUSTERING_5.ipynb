{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### **Q1. What is a contingency matrix, and how is it used to evaluate the performance of a classification model?**\n",
        "#####[Ans]\n",
        "- **Contingency Matrix**: Also known as a confusion matrix, it is a table used to evaluate the performance of a classification model by comparing the true labels to the predicted labels.\n",
        "\n",
        "**Structure**:\n",
        "- Rows represent the true labels.\n",
        "- Columns represent the predicted labels.\n",
        "- Each cell counts the number of instances for a specific true-predicted label pair.\n",
        "\n",
        "**Uses**:\n",
        "- Helps compute metrics like accuracy, precision, recall, and F1-score.\n",
        "- Provides insights into specific types of classification errors.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q2. How is a pair confusion matrix different from a regular confusion matrix, and why might it be useful in certain situations?**\n",
        "#####[Ans]\n",
        "- **Pair Confusion Matrix**: Used to evaluate clustering or other unsupervised learning tasks by comparing pairs of points instead of individual points.\n",
        "\n",
        "**Difference**:\n",
        "- Regular confusion matrix compares true vs. predicted labels.\n",
        "- Pair confusion matrix evaluates the agreement of pairwise assignments (e.g., same cluster or different cluster).\n",
        "\n",
        "**Usefulness**:\n",
        "- Useful for clustering problems where individual labels may not be directly comparable, but relationships between data points matter.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q3. What is an extrinsic measure in the context of natural language processing, and how is it typically used to evaluate the performance of language models?**\n",
        "#####[Ans]\n",
        "- **Extrinsic Measure**: Evaluates the performance of a language model based on its effectiveness in a specific downstream task (e.g., sentiment analysis, machine translation).\n",
        "\n",
        "**Example**:\n",
        "- Using a language model to improve the accuracy of a sentiment analysis classifier and measuring the accuracy on a test set.\n",
        "\n",
        "**Purpose**:\n",
        "- Ensures that the model performs well in real-world applications.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q4. What is an intrinsic measure in the context of machine learning, and how does it differ from an extrinsic measure?**\n",
        "#####[Ans]\n",
        "- **Intrinsic Measure**: Evaluates a model based on internal metrics or properties without considering downstream tasks.\n",
        "\n",
        "**Example**:\n",
        "- Measuring perplexity or BLEU score for language models.\n",
        "\n",
        "**Difference**:\n",
        "- Intrinsic measures focus on the model itself, while extrinsic measures evaluate its performance on external tasks.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q5. What is the purpose of a confusion matrix in machine learning, and how can it be used to identify strengths and weaknesses of a model?**\n",
        "#####[Ans]\n",
        "- **Purpose**: To evaluate classification model performance by analyzing true positives, true negatives, false positives, and false negatives.\n",
        "\n",
        "**Usage**:\n",
        "- Identify class-specific performance issues (e.g., high false positives in a specific class).\n",
        "- Compute detailed metrics like precision, recall, and F1-score.\n",
        "- Provide insights into model strengths and weaknesses (e.g., biased towards certain classes).\n",
        "\n",
        "---\n",
        "\n",
        "### **Q6. What are some common intrinsic measures used to evaluate the performance of unsupervised learning algorithms, and how can they be interpreted?**\n",
        "#####[Ans]\n",
        "- **Common Intrinsic Measures**:\n",
        "  1. **Silhouette Coefficient**: Measures cluster compactness and separation. Ranges from -1 (poor clustering) to 1 (ideal clustering).\n",
        "  2. **Davies-Bouldin Index**: Measures cluster scatter and separation. Lower values indicate better clustering.\n",
        "  3. **Calinski-Harabasz Index**: Ratio of between-cluster variance to within-cluster variance. Higher values indicate better clustering.\n",
        "\n",
        "**Interpretation**:\n",
        "- Provide insights into cluster quality without ground truth labels.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q7. What are some limitations of using accuracy as a sole evaluation metric for classification tasks, and how can these limitations be addressed?**\n",
        "#####[Ans]\n",
        "- **Limitations**:\n",
        "  1. **Imbalanced Datasets**: Accuracy can be misleading when one class dominates.\n",
        "  2. **Does Not Differentiate Errors**: Treats all misclassifications equally, ignoring false positives vs. false negatives.\n",
        "\n",
        "- **Solutions**:\n",
        "  1. Use metrics like precision, recall, and F1-score for a detailed evaluation.\n",
        "  2. Employ ROC-AUC or PR-AUC for imbalanced datasets.\n",
        "  3. Analyze the confusion matrix for class-specific performance insights.\n"
      ],
      "metadata": {
        "id": "RCugiXyuIEFL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktv0vg_9HEvo"
      },
      "outputs": [],
      "source": []
    }
  ]
}