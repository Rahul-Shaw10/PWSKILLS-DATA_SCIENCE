{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Q1: What are Eigenvalues and Eigenvectors? How are they related to the Eigen-Decomposition approach? Explain with an example.**\n",
        "\n",
        "- **Eigenvalues**: Scalars representing how much a transformation stretches or shrinks vectors in a given direction.\n",
        "\n",
        "- **Eigenvectors**: Non-zero vectors whose direction remains unchanged under a transformation.\n",
        "\n",
        "They satisfy the equation:  \n",
        "$$\n",
        "A \\mathbf{v} = \\lambda \\mathbf{v}\n",
        "$$\n",
        "\n",
        "Where $A $ is the matrix, $\\mathbf{v}$ is the eigenvector, and $\\lambda$ is the eigenvalue.\n",
        "\n",
        "**Relation to Eigen-Decomposition**: Eigen-Decomposition expresses a matrix \\( A \\) as:  \n",
        "$$\n",
        "A = Q \\Lambda Q^{-1}\n",
        "$$\n",
        "Where:  \n",
        "- $ Q $: Matrix of eigenvectors.  \n",
        "- $\\Lambda $: Diagonal matrix of eigenvalues.\n",
        "\n",
        "**Example**:  \n",
        "For $ A = \\begin{bmatrix} 4 & 1 \\\\ 2 & 3 \\end{bmatrix} $, eigenvalues are $ \\lambda_1 = 5 $, $\\lambda_2 = 2 $, and corresponding eigenvectors can be calculated.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q2: What is eigen decomposition and what is its significance in linear algebra?**\n",
        "\n",
        "**Eigen-Decomposition** is the factorization of a square matrix \\( A \\) into eigenvalues and eigenvectors.  \n",
        "$$\n",
        "A = Q \\Lambda Q^{-1}\n",
        "$$\n",
        "\n",
        "**Significance**:  \n",
        "- Simplifies matrix computations (e.g., matrix powers, exponentiation).  \n",
        "- Provides insight into transformations (e.g., scaling, rotation).  \n",
        "- Widely used in dimensionality reduction (e.g., PCA).\n",
        "\n",
        "---\n",
        "\n",
        "### **Q3: What are the conditions that must be satisfied for a square matrix to be diagonalizable using the Eigen-Decomposition approach? Provide a brief proof to support your answer.**\n",
        "\n",
        "**Conditions**:  \n",
        "1. $ A $ must have $ n $ linearly independent eigenvectors.  \n",
        "2. $ A $ is diagonalizable if it has distinct eigenvalues or is symmetric.\n",
        "\n",
        "**Proof Outline**:  \n",
        "If $ A $ has $ n $ linearly independent eigenvectors, they form the columns of $ Q $. Then:  \n",
        "$$\n",
        "Q^{-1} A Q = \\Lambda\n",
        "$$\n",
        "Diagonalizing $ A $.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q4: What is the significance of the spectral theorem in the context of the Eigen-Decomposition approach? How is it related to the diagonalizability of a matrix? Explain with an example.**\n",
        "\n",
        "The **Spectral Theorem** states that any symmetric matrix is diagonalizable by an orthogonal matrix $Q $:  \n",
        "$$\n",
        "A = Q \\Lambda Q^T\n",
        "$$  \n",
        "\n",
        "**Significance**:  \n",
        "- Guarantees diagonalizability for symmetric matrices.  \n",
        "- Simplifies computations in PCA, quantum mechanics, etc.\n",
        "\n",
        "**Example**:  \n",
        "For 4 A = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix} $, eigenvalues and eigenvectors can diagonalize $ A $ with an orthogonal matrix.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q5: How do you find the eigenvalues of a matrix and what do they represent?**\n",
        "\n",
        "1. Solve the characteristic equation:  \n",
        "$$\n",
        "\\det(A - \\lambda I) = 0\n",
        "$$  \n",
        "2. Solve for $\\lambda $, the eigenvalues.\n",
        "\n",
        "**Representation**:  \n",
        "Eigenvalues represent scaling factors along eigenvector directions during a transformation.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q6: What are eigenvectors and how are they related to eigenvalues?**\n",
        "\n",
        "Eigenvectors are vectors whose direction remains unchanged under transformation by $ A $, scaled by eigenvalues:  \n",
        "$$\n",
        "A \\mathbf{v} = \\lambda \\mathbf{v}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### **Q7: Can you explain the geometric interpretation of eigenvectors and eigenvalues?**\n",
        "\n",
        "- **Eigenvectors**: Represent invariant directions under a transformation.  \n",
        "- **Eigenvalues**: Represent scaling factors applied along eigenvector directions.\n",
        "\n",
        "**Example**:  \n",
        "In a 2D plane, an eigenvector might represent a direction of stretching, while the eigenvalue indicates how much it is stretched.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q8: What are some real-world applications of eigen decomposition?**\n",
        "\n",
        "1. **Principal Component Analysis (PCA)**: Reducing data dimensionality.  \n",
        "2. **Vibration Analysis**: Identifying natural frequencies.  \n",
        "3. **Image Compression**: Representing images using fewer components.  \n",
        "4. **Quantum Mechanics**: Solving Schr√∂dinger's equation.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q9: Can a matrix have more than one set of eigenvectors and eigenvalues?**\n",
        "\n",
        "- **Unique eigenvalues**: Correspond to distinct eigenvectors.  \n",
        "- **Repeated eigenvalues**: May have infinitely many eigenvectors (spanning a subspace).\n",
        "\n",
        "---\n",
        "\n",
        "### **Q10: In what ways is the Eigen-Decomposition approach useful in data analysis and machine learning? Discuss at least three specific applications or techniques that rely on Eigen-Decomposition.**\n",
        "\n",
        "1. **Dimensionality Reduction (PCA)**: Identifying principal components to reduce features.  \n",
        "2. **Spectral Clustering**: Leveraging eigenvalues for graph-based clustering.  \n",
        "3. **Latent Semantic Analysis (LSA)**: Analyzing relationships in textual data.\n"
      ],
      "metadata": {
        "id": "X2CKzOQSz8Ez"
      }
    }
  ]
}